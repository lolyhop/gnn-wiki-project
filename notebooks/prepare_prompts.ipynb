{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "903c2ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import json\n",
    "import typing as tp\n",
    "import pandas as pd\n",
    "from textwrap import shorten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3b367c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a classification assistant. Your task is to choose which categories from a provided list are semantically relevant to a given Wikipedia article.\n",
      "\n",
      "You will be given:\n",
      "1) The article title\n",
      "2) A short text/summary describing the article\n",
      "3) A list of candidate categories, indexed from 0 to N-1\n",
      "\n",
      "Your goal:\n",
      "- Select ALL categories that apply to the article's meaning or topic.\n",
      "- You MUST output ONLY a JSON list of integers (category indices).\n",
      "- Do NOT output category names or explanations.\n",
      "- If no categories fit, return an empty list `[]`.\n",
      "\n",
      "Guidelines:\n",
      "- Choose categories representing the main topic, not formatting/maintenance tags.\n",
      "- Prefer domain-level, thematic, or conceptual categories over meta/technical/admin ones.\n",
      "- Ignore categories describing Wikipedia housekeeping, templates, flags, stubs, etc.\n",
      "\n",
      "Return ONLY:\n",
      "A JSON list of selected indices.\n",
      "Example:\n",
      "[2, 5, 6]\n",
      "\n",
      "\n",
      "ARTICLE TITLE:\n",
      "Backpropagation\n",
      "\n",
      "ARTICLE SUMMARY:\n",
      "{{text}}\n",
      "\n",
      "CANDIDATE CATEGORIES (indexed):\n",
      "{{indexed_categories}}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TEMPLATE = open(\"../prompts/filter_labels.txt\", \"r\").read()\n",
    "\n",
    "categories = [\n",
    "    \"Machine learning\",\n",
    "    \"Biology\",\n",
    "    \"Ancient Rome\",\n",
    "    \"Neural networks\",\n",
    "    \"Psychology\",\n",
    "]\n",
    "\n",
    "indexed = \"\\n\".join(f\"{i}: {c}\" for i,c in enumerate(categories))\n",
    "\n",
    "prompt = PROMPT_TEMPLATE.replace(\"{{title}}\", \"Backpropagation\") \\\n",
    "                        .replace(\"{{summary}}\", \"Backpropagation is an algorithm ...\") \\\n",
    "                        .replace(\"{{indexed_category_list}}\", indexed)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35197201",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = (\n",
    "    PROMPT_TEMPLATE\n",
    "    .replace(\"{{title}}\", \"{title}\")\n",
    "    .replace(\"{{text}}\", \"{text}\")\n",
    "    .replace(\"{{indexed_categories}}\", \"{indexed_categories}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0025e2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = json.loads(open(\"../data/wiki_graph_dedup.json\", \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d18a594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(node: tp.Dict[str, tp.Any], max_text_len: int = 2000) -> str:\n",
    "    title = node.get(\"title\", \"\").strip()\n",
    "    text = node.get(\"text\", \"\").strip()\n",
    "    text = shorten(text, width=max_text_len, placeholder=\" ...\")\n",
    "\n",
    "    categories = node.get(\"categories\", [])\n",
    "    indexed = \"\\n\".join(f\"{i}: {c}\" for i, c in enumerate(categories))\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        title=title,\n",
    "        text=text,\n",
    "        indexed_categories=indexed\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "462cccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = graph[\"nodes\"]\n",
    "\n",
    "prompts = {}\n",
    "for title, node in nodes.items():\n",
    "    prompts[title] = build_prompt(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2cbbadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(prompts.items(), columns=[\"title\", \"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54f59fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a classification assistant. Your task is to choose which categories from a provided list are semantically relevant to a given Wikipedia article.\n",
      "\n",
      "You will be given:\n",
      "1) The article title\n",
      "2) A short text/summary describing the article\n",
      "3) A list of candidate categories, indexed from 0 to N-1\n",
      "\n",
      "Your goal:\n",
      "- Select ALL categories that apply to the article's meaning or topic.\n",
      "- You MUST output ONLY a JSON list of integers (category indices).\n",
      "- Do NOT output category names or explanations.\n",
      "- If no categories fit, return an empty list `[]`.\n",
      "\n",
      "Guidelines:\n",
      "- Choose categories representing the main topic, not formatting/maintenance tags.\n",
      "- Prefer domain-level, thematic, or conceptual categories over meta/technical/admin ones.\n",
      "- Ignore categories describing Wikipedia housekeeping, templates, flags, stubs, etc.\n",
      "\n",
      "Return ONLY:\n",
      "A JSON list of selected indices.\n",
      "Example:\n",
      "[2, 5, 6]\n",
      "\n",
      "\n",
      "ARTICLE TITLE:\n",
      "2018 Google data breach\n",
      "\n",
      "ARTICLE SUMMARY:\n",
      "The 2018 Google data breach was a major data privacy scandal in which the Google+ API exposed the private data of over five hundred thousand users. Google+ managers first noticed harvesting of personal data in March 2018, during a review following the Facebook–Cambridge Analytica data scandal. The bug, despite having been fixed immediately, exposed the private data of approximately 500,000 Google+ users to the public. Google did not reveal the leak to the network's users. In November 2018, another data breach occurred following an update to the Google+ API. Although Google found no evidence of failure, approximately 52.5 million personal profiles were potentially exposed. In August 2019, Google declared a shutdown of Google+ due to low use and technological challenges. Overview of Google+ Google+ was launched in June 2011 as an invite-only social network, but was opened for public access later in the year. It was managed by Vic Gundotra. Similar to Facebook, Google+ also included key features Circles, Hangouts and Sparks. Circles let users personalize their social groups by sorting friends into different categories. Once allowed into a Circle, users could regulate information in their individual spaces. Hangouts included video chatting and instant messaging between users. Sparks allowed Google to track users' past searches to find news and content related to their interests. Google+ was linked to other Google services, such as YouTube, Google Drive and Gmail, giving it access to roughly 2 billion user accounts. However, less than 400 million consumers actively used Google+, with 90% of those users using it for less than five seconds. The breaches In March 2018, Google developers found a data breach within the Google+ People API in which external apps acquired access to Profile fields that were not marked as public. According to The Wall Street Journal, Google didn’t disclose the breach when it was first discovered in March to avoid regulatory scrutiny and ...\n",
      "\n",
      "CANDIDATE CATEGORIES (indexed):\n",
      "0: Category:2018 data breaches\n",
      "1: Category:2018 in computing\n",
      "2: Category:Articles with short description\n",
      "3: Category:Computer security\n",
      "4: Category:Google\n",
      "5: Category:Short description matches Wikidata\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[123]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36b25147",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/filtering_prompts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8238c42a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Processing the LLM-as-a-judge result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "111ae900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>prompt</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>deepseek_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>You are a classification assistant. Your task ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Data binding</td>\n",
       "      <td>&lt;think&gt;\\nAlright, so I've got this task to fig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>You are a classification assistant. Your task ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Intelligent agent</td>\n",
       "      <td>&lt;think&gt;\\nAlright, let's tackle this classifica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>You are a classification assistant. Your task ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Self-management (computer science)</td>\n",
       "      <td>&lt;think&gt;\\nAlright, let's tackle this classifica...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1                                             prompt  \\\n",
       "0             0  You are a classification assistant. Your task ...   \n",
       "1             1  You are a classification assistant. Your task ...   \n",
       "2             2  You are a classification assistant. Your task ...   \n",
       "\n",
       "   Unnamed: 0                               title  \\\n",
       "0           0                        Data binding   \n",
       "1           1                   Intelligent agent   \n",
       "2           2  Self-management (computer science)   \n",
       "\n",
       "                                     deepseek_answer  \n",
       "0  <think>\\nAlright, so I've got this task to fig...  \n",
       "1  <think>\\nAlright, let's tackle this classifica...  \n",
       "2  <think>\\nAlright, let's tackle this classifica...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/filtered_categories.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25778370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse <think> section\n",
    "def extract_list(answer):\n",
    "    answer = answer.split(\"</think>\")[-1].strip()\n",
    "    try:\n",
    "        match = re.search(r\"\\[.*?\\]\", answer, re.DOTALL)\n",
    "        if match:\n",
    "            return ast.literal_eval(match.group())\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "df[\"valid_idx\"] = df[\"deepseek_answer\"].apply(extract_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92ab6f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = json.loads(open(\"../data/wiki_graph_dedup.json\", \"r\").read()) # read the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57b67828",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_KEYWORDS = [\n",
    "    \"births\", \"deaths\", \"living people\", \"films\", \"television\", \n",
    "    \"musicians\", \"sports\", \"movies\", \"establishments\", \"schools\",\n",
    "    \"universities\", \"museums\", \"politicians\", \"countries\", \n",
    "    \"historical\", \"events\", \"wikipedia\", \"organizations\", \"politics\",\n",
    "    \"latin-language\", \"ancient greek\", \"french-language\", \"japanese-language\",\n",
    "    \"german-language\", \"chinese-language\", \"spanish-language\", \"russian-language\",\n",
    "    \"pages including recorded pronunciations\", \"pages with plain ipa\",\n",
    "    \"pages with french ipa\", \"pages using the phonos extension\",\n",
    "    \"american inventions\", \"elections using electoral votes\", \"united states supreme court cases\",\n",
    "    \"disambiguation\", \"cities in\", \"unincorporated communities\", \"nations at\", \"journals\",\n",
    "    \"academy\", \"academia\", \"schools\", \"universities\", \"sports\", \"olympics\", \"countries\",\n",
    "    \"english-language\", \"french-language\", \"german-language\", \"spanish-language\", \"latin-language\",\n",
    "    \"russian-language\", \"japanese-language\", \"chinese-language\"\n",
    "]\n",
    "\n",
    "def is_noise_category(category: str) -> bool:\n",
    "    cat_lower: str = category.lower()\n",
    "    return any(keyword in cat_lower for keyword in NOISE_KEYWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b3b3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.iterrows():\n",
    "    title = row['title']\n",
    "    valid_idx = row['valid_idx']  # valid categories that are meaningful\n",
    "\n",
    "    # Remove noisy categories\n",
    "    all_categories = graph['nodes'][title]['categories']\n",
    "    filtered_categories = [all_categories[i] for i in valid_idx if i < len(all_categories)]\n",
    "    graph['nodes'][title]['categories'] = filtered_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa81b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Removed 6616 noisy nodes. Remaining nodes: 7934\n"
     ]
    }
   ],
   "source": [
    "titles_to_remove = []\n",
    "\n",
    "for title, node in graph['nodes'].items():\n",
    "    categories = node.get('categories', [])\n",
    "    if any(is_noise_category(cat) for cat in categories):\n",
    "        titles_to_remove.append(title)\n",
    "\n",
    "for title in titles_to_remove:\n",
    "    del graph['nodes'][title]\n",
    "\n",
    "print(f\"[INFO] Removed {len(titles_to_remove)} noisy nodes. Remaining nodes: {len(graph['nodes'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1515bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Remaining edges after cleanup: 7859\n"
     ]
    }
   ],
   "source": [
    "remaining_nodes = set(graph['nodes'].keys())\n",
    "\n",
    "graph['edges'] = [\n",
    "    (src, dst) for src, dst in graph['edges']\n",
    "    if src in remaining_nodes and dst in remaining_nodes\n",
    "]\n",
    "\n",
    "print(f\"[INFO] Remaining edges after cleanup: {len(graph['edges'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b74be418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save graph with filtered categories\n",
    "with open(\"../data/wiki_graph_filtered_v2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(graph, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
